{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "op5dMsL9MSqY",
        "colab_type": "code",
        "outputId": "b661e8cb-7b21-4ab8-b38a-07ea8a520e06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GafBLbHwI6hI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import pandas as pd\n",
        "eps = np.finfo(float).eps\n",
        "import operator\n",
        "from tabulate import tabulate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "import random\n",
        "import sys\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5LtEa8WoLm6f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RdXdHYDJI6hM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data = pd.read_csv(\"/content/gdrive/My Drive/SMAI_Assignments_colab/apparel-trainval.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ue7RdocmI6hQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_data():\n",
        "#     data = pd.read_csv(path)\n",
        "    y = data[['label']].values\n",
        "    X = data.drop(data.columns[0], axis = 1)\n",
        "    X_normalized = StandardScaler().fit_transform(X)\n",
        "    X_train, X_validate, y_train, y_validate = train_test_split(X_normalized, y, test_size = 0.2, random_state=42)\n",
        "    return X_train, X_validate, y_train, y_validate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "O06_u7a7I6hT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X_train, X_validate, y_train, y_validate = process_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4U6OVaoII6hZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class neural_network(object):\n",
        "    def __init__(self, activation_func, activation_func_derivative, alpha, ILS, OLS, hiddenLayers):\n",
        "        self.weightMatrix = []\n",
        "        self.allInputs = []\n",
        "        self.allOutputs = []\n",
        "        self.inputLayerSize = ILS\n",
        "        self.outputLayerSize = OLS\n",
        "        self.hiddenLayers = hiddenLayers\n",
        "        self.activation_func = activation_func\n",
        "        self.activation_func_derivative = activation_func_derivative\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        if(len(self.hiddenLayers) == 0):\n",
        "            bound = np.sqrt(1./(self.inputLayerSize))\n",
        "            self.weightMatrix.append(np.random.uniform(-bound, bound, (self.inputLayerSize, self.outputLayerSize)))\n",
        "        else:\n",
        "            bound = np.sqrt(1./(self.hiddenLayers[0]))\n",
        "            self.weightMatrix.append(np.random.uniform(-bound, bound, (self.inputLayerSize, self.hiddenLayers[0])))\n",
        "            for x in range(1,len(self.hiddenLayers)):\n",
        "                bound = np.sqrt(1./(self.hiddenLayers[x]))\n",
        "                self.weightMatrix.append(np.random.uniform(-bound, bound, (self.hiddenLayers[x-1], self.hiddenLayers[x])))\n",
        "            self.weightMatrix.append(np.random.uniform(-bound, bound, (self.hiddenLayers[len(self.hiddenLayers)-1], self.outputLayerSize)))\n",
        "    \n",
        "    def forwardPropagation(self, Input):\n",
        "        self.allInputs = []\n",
        "        self.allOutputs = []\n",
        "        for i in range(0, len(self.weightMatrix)):\n",
        "            \n",
        "            if(i == 0):\n",
        "                self.allInputs.append(np.matmul(Input,self.weightMatrix[0]))\n",
        "            else:\n",
        "                self.allInputs.append(np.matmul(self.allOutputs[i-1],self.weightMatrix[i]))\n",
        "            \n",
        "            if (i == len(self.weightMatrix)-1):\n",
        "                self.allOutputs.append(stable_softmax(self.allInputs[i]))\n",
        "            else:\n",
        "                self.allOutputs.append(self.activation_func(self.allInputs[i]))\n",
        "        return self.allOutputs\n",
        "    \n",
        "    def backPropagation(self, encoded, input):\n",
        "        delta_list = [0]*len(self.weightMatrix)\n",
        "        for i in reversed(range(len(self.weightMatrix))):\n",
        "            if(i == len(self.weightMatrix) - 1):\n",
        "                delta = np.multiply((self.allOutputs[i] - encoded), softmax_derivative(self.allInputs[i]))\n",
        "            else:\n",
        "                delta = np.multiply(np.dot(delta_list[i+1], self.weightMatrix[i+1].T), self.activation_func_derivative(self.allInputs[i]))\n",
        "            delta_list[i] = delta\n",
        "        for i in range(0, len(self.weightMatrix)):\n",
        "            if(i == 0):\n",
        "                self.weightMatrix[i] -= self.alpha * np.dot(input.T, delta_list[0])\n",
        "            else:\n",
        "                self.weightMatrix[i] -= self.alpha * np.dot(self.allOutputs[i-1].T, delta_list[i])\n",
        "                \n",
        "    def predictionOnTest(model_test, data):\n",
        "      outputs = model_test.forwardPropagation(data)\n",
        "      predictions = np.argmax(outputs[-1], axis=1)\n",
        "      return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4xc6ImMI6hf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def oneHotEncode(data):\n",
        "    n_classes = len(np.unique(data))\n",
        "    targets = np.array(data).reshape(-1)\n",
        "    return np.eye(n_classes)[targets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z7mKBPi1I6hi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# oneHot = oneHotEncode(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjBus-NPI6hn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def crossEntropy(prediction, target, epsilon=1e-12):\n",
        "    prediction = np.clip(prediction, epsilon, 1. - epsilon)\n",
        "    N = prediction.shape[0]\n",
        "    c_entropy = -np.sum(target * np.log(prediction+1e-9))/N\n",
        "    return c_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cgFhBniXI6hr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ReLU_derivative(matrix):\n",
        "    matrix[matrix > 0] = 1\n",
        "    matrix[matrix <= 0] = 0\n",
        "    return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3dNB_pVpI6hv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " def ReLU(matrix):\n",
        "        return np.maximum(matrix, 0.0, matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRe5pw-iXIEh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(matrix):\n",
        "  sigm = 1. / (1. + np.exp(-matrix))\n",
        "  return sigm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Yen6jdkYDaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(matrix):\n",
        "  sigm = sigmoid(matrix)\n",
        "  return sigm * (1. - sigm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6B-_n8tbX5Eh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tan_h(matrix):\n",
        "  return np.tanh(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3YDFI0gdYbGQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tan_h_derivative(matrix):\n",
        "  return 1.0 - np.tanh(matrix)**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJgpUza5I6hy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stable_softmax(matrix):\n",
        "        shift_x = matrix - np.array([np.max(matrix, axis=1)]).T\n",
        "        exps = np.exp(shift_x)\n",
        "        return exps / np.array([np.sum(exps, axis=1)]).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5HTb9fMSI6h1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax_derivative(matrix):\n",
        "    temp = stable_softmax(matrix)\n",
        "    return temp*(1 - temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VHrSXoc0U8eF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainOnRelu(model_relu, epochs):\n",
        "    error_list_relu = []\n",
        "    iter_list = []\n",
        "    for i in range(epochs):\n",
        "      print(\"relu : \", i)\n",
        "      outputs = model_relu.forwardPropagation(X_train)\n",
        "      c_entropy = crossEntropy(outputs[-1], oneHot)\n",
        "      error_list_relu.append(c_entropy)\n",
        "      iter_list.append(i)\n",
        "      model_relu.backPropagation(oneHot, X_train)\n",
        "    return error_list_relu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9DJBUjRVqEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainOnSigmoid(model_sigmoid, epochs):\n",
        "    error_list_sigmoid = []\n",
        "    iter_list = []\n",
        "    for i in range(epochs):\n",
        "      print(\"sigmoid : \", i)\n",
        "      outputs = model_sigmoid.forwardPropagation(X_train)\n",
        "      c_entropy = crossEntropy(outputs[-1], oneHot)\n",
        "      error_list_sigmoid.append(c_entropy)\n",
        "      iter_list.append(i)\n",
        "      model_sigmoid.backPropagation(oneHot, X_train)\n",
        "    return error_list_sigmoid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8sFg4tuuV_q6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainOnTanh(model_tanh, epochs):\n",
        "    error_list_tanh = []\n",
        "    iter_list = []\n",
        "    for i in range(epochs):\n",
        "      print(\"tanh : \", i)\n",
        "      outputs = model_tanh.forwardPropagation(X_train)\n",
        "      c_entropy = crossEntropy(outputs[-1], oneHot)\n",
        "      error_list_tanh.append(c_entropy)\n",
        "      iter_list.append(i)\n",
        "      model_tanh.backPropagation(oneHot, X_train)\n",
        "    return error_list_tanh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e0aHAbE8XNCe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def miniBatchTrain(miniBatchModel, batchSize, epochs):\n",
        "    batchCount = int(X_train.shape[0] / batchSize)\n",
        "    miniBatchError = []\n",
        "    for iters in range(epochs):\n",
        "      print(\"miniBatch : \", iters)\n",
        "      startIndex = 0\n",
        "      endIndex = batchSize\n",
        "      for batchNo in range(batchCount):\n",
        "        X_miniBatch = X_train[startIndex:endIndex]\n",
        "        outputs = miniBatchModel.forwardPropagation(X_miniBatch)\n",
        "        y_miniBatch = oneHot[startIndex:endIndex]\n",
        "        if(batchNo == batchCount - 1):\n",
        "          c_entropy = crossEntropy(outputs[-1], y_miniBatch)\n",
        "          miniBatchError.append(c_entropy)\n",
        "        miniBatchModel.backPropagation(y_miniBatch, X_miniBatch)\n",
        "        startIndex = endIndex\n",
        "        endIndex += batchSize\n",
        "    return miniBatchError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o80lhw8EbRvn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plotBatch(error_list_relu, error_list_sigmoid, error_list_tanh, iter_list):\n",
        "    fig, ax = plt.subplots(figsize=(12,6))\n",
        "    ax.scatter(iter_list, error_list_relu, color=\"blue\",marker = '.', s= 30, label = 'ReLU')\n",
        "    ax.scatter(iter_list, error_list_sigmoid, color=\"black\",marker = '.', s= 30, label = 'Sigmoid')\n",
        "    ax.scatter(iter_list, error_list_tanh, color=\"red\",marker = '.', s= 30, label = 'Tanh')\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Cross Entropy Error\")\n",
        "    plt.title(\"Iterations vs Model Cost : Batch Mode\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8koX_XIbm9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plotMiniBatch(error_list_relu, error_list_sigmoid, error_list_tanh, iter_list):\n",
        "    fig1, ax1 = plt.subplots(figsize=(12,6))\n",
        "    ax1.scatter(iter_list, error_list_relu, color=\"blue\",marker = '.', s= 30, label = 'ReLU')\n",
        "    ax1.scatter(iter_list, error_list_sigmoid, color=\"black\",marker = '.', s= 30, label = 'Sigmoid')\n",
        "    ax1.scatter(iter_list, error_list_tanh, color=\"red\",marker = '.', s= 30, label = 'Tanh')\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Cross Entropy Error\")\n",
        "    plt.title(\"Iterations vs Model Cost : Mini Batch Mode\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P4efG_GXPvna",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(trainType):\n",
        "    hiddenLayers = [60,60]\n",
        "    input_dim = X_train.shape[1]\n",
        "    output_dim = len(np.unique(y_train))\n",
        "    alpha_relu = 0.0001\n",
        "    alpha_sigmoid = 0.0001\n",
        "    alpha_tanh = 0.0001\n",
        "    \n",
        "    if(trainType == \"batch\"):\n",
        "      model_relu = neural_network(ReLU, ReLU_derivative, alpha_relu, input_dim, output_dim, hiddenLayers)\n",
        "      model_sigmoid = neural_network(sigmoid, sigmoid_derivative, alpha_sigmoid, input_dim, output_dim, hiddenLayers)\n",
        "      model_tanh = neural_network(tan_h, tan_h_derivative, alpha_tanh, input_dim, output_dim, hiddenLayers)\n",
        "      epochs = 500\n",
        "      relu_error = trainOnRelu(model_relu, epochs)\n",
        "      sigmoid_error = trainOnSigmoid(model_sigmoid, epochs)\n",
        "      tanh_error = trainOnTanh(model_tanh, epochs)\n",
        "      iter_list = range(0, epochs)\n",
        "      plotBatch(relu_error, sigmoid_error, tanh_error, iter_list)\n",
        "      \n",
        "      print(\"Minimum Error by ReLU : \", np.min(relu_error))\n",
        "      print(\"Minimum Error by Tanh : \", np.min(tanh_error))\n",
        "      print(\"Minimum Error by Sigmoid : \", np.min(sigmoid_error))\n",
        "      \n",
        "      \n",
        "      print(\"\\n\\nACCURACY USING BATCH MODE: \")\n",
        "      # Relu accuracy\n",
        "      val_output = model_relu.forwardPropagation(X_validate)\n",
        "      predictions = np.argmax(val_output[-1], axis=1)\n",
        "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "      print(\"RELU : \", (np.mean(predictions == y_val))*100)\n",
        "\n",
        "      # Sigmoid accuracy\n",
        "      val_output = model_sigmoid.forwardPropagation(X_validate)\n",
        "      predictions = np.argmax(val_output[-1], axis=1)\n",
        "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "      print(\"SIGMOID : \", (np.mean(predictions == y_val))*100)\n",
        "\n",
        "      # Tanh accuracy\n",
        "      val_output = model_tanh.forwardPropagation(X_validate)\n",
        "      predictions = np.argmax(val_output[-1], axis=1)\n",
        "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "      print(\"TANH : \", (np.mean(predictions == y_val))*100)\n",
        "      \n",
        "      # saving weights\n",
        "      weight_batch_relu = model_relu.weightMatrix\n",
        "      np.save('weights_batch_relu', weight_batch_relu)\n",
        "      \n",
        "      weight_batch_sigmoid = model_sigmoid.weightMatrix\n",
        "      np.save('weights_batch_sigmoid', weight_batch_sigmoid)\n",
        "      \n",
        "      weight_batch_tanh = model_tanh.weightMatrix\n",
        "      np.save('weights_batch_tanh', weight_batch_tanh)\n",
        "      \n",
        "      \n",
        "      \n",
        "    elif(trainType == \"miniBatch\"):\n",
        "      iters = 500\n",
        "      batchSize = 32\n",
        "      iter_list = range(0, iters)\n",
        "      model_miniBatch_relu = neural_network(ReLU, ReLU_derivative, alpha_relu, input_dim, output_dim, hiddenLayers)\n",
        "      model_miniBatch_sigmoid = neural_network(sigmoid, sigmoid_derivative, alpha_sigmoid, input_dim, output_dim, hiddenLayers)\n",
        "      model_miniBatch_tanh = neural_network(tan_h, tan_h_derivative, alpha_tanh, input_dim, output_dim, hiddenLayers)\n",
        "#       miniBatchError_relu = miniBatchTrain(model_miniBatch_relu, batchSize, iters)\n",
        "#       miniBatchError_sigmoid = miniBatchTrain(model_miniBatch_sigmoid, batchSize, iters)\n",
        "      miniBatchError_tanh = miniBatchTrain(model_miniBatch_tanh, batchSize, iters)\n",
        "      fig1, ax1 = plt.subplots(figsize=(12,6))\n",
        "      ax1.scatter(iter_list, miniBatchError_tanh, color=\"blue\",marker = '.', s= 30, label = 'Tanh')\n",
        "      plt.legend()\n",
        "      plt.xlabel(\"Iterations\")\n",
        "      plt.ylabel(\"Model Loss\")\n",
        "      plt.title(\"Iterations vs Model Loss : Mini Batch Mode\")\n",
        "      plt.show()\n",
        "      \n",
        "      print(\"\\n\\nACCURACY USING MINI BATCH: \")\n",
        "#       val_output = model_miniBatch_relu.forwardPropagation(X_validate)\n",
        "#       predictions = np.argmax(val_output[-1], axis=1)\n",
        "#       y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "#       print(\"RELU : \", (np.mean(predictions == y_val))*100)\n",
        "      \n",
        "#       val_output = model_miniBatch_sigmoid.forwardPropagation(X_validate)\n",
        "#       predictions = np.argmax(val_output[-1], axis=1)\n",
        "#       y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "#       print(\"SIGMOID : \", (np.mean(predictions == y_val))*100)\n",
        "      \n",
        "      val_output = model_miniBatch_tanh.forwardPropagation(X_validate)\n",
        "      predictions = np.argmax(val_output[-1], axis=1)\n",
        "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "      print(\"TANH : \", (np.mean(predictions == y_val))*100)\n",
        "      \n",
        "      # saving weights\n",
        "#       weight_mini_batch_relu = model_miniBatch_relu.weightMatrix\n",
        "#       np.save('weights_mini_batch_relu', weight_mini_batch_relu)\n",
        "      \n",
        "#       weight_mini_batch_sigmoid = model_miniBatch_sigmoid.weightMatrix\n",
        "#       np.save('weights_mini_batch_sigmoid', weight_mini_batch_sigmoid)\n",
        "      \n",
        "      weight_mini_batch_tanh = model_miniBatch_tanh.weightMatrix\n",
        "      np.save('/content/gdrive/My Drive/SMAI_Assignments_colab/weights_mini_batch_tanh', weight_mini_batch_tanh)\n",
        "\n",
        "    else:\n",
        "      print(\"Wrong Training Method Provided\")\n",
        "      sys.exit()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LoRLpaO0Jnve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(test_data):\n",
        "  alpha_tanh = 0.0001\n",
        "  input_dim = test_data.shape[1]\n",
        "  output_dim = 10\n",
        "  hiddenLayers = [60, 60]\n",
        "  model_test = neural_network(tan_h, tan_h_derivative, alpha_tanh, input_dim, output_dim, hiddenLayers)\n",
        "  weights = np.load('/content/gdrive/My Drive/SMAI_Assignments_colab/weights_mini_batch_tanh.npy')\n",
        "  weights = weights.tolist()\n",
        "  model_test.weightMatrix = weights\n",
        "  predictions = model_test.predictionOnTest(test_data)\n",
        "  fileData = [[i] for i in predictions]\n",
        "  with open('/content/gdrive/My Drive/SMAI_Assignments_colab/2018201057_prediction.csv', 'w') as outputFile:\n",
        "    writer = csv.writer(outputFile)\n",
        "    writer.writerows(fileData)\n",
        "  outputFile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "_1cwoUuMI6h4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hiddenLayers = [60,60]\n",
        "# input_dim = X_train.shape[1]\n",
        "# output_dim = len(np.unique(y_train))\n",
        "# alpha_relu = 0.0001\n",
        "# alpha_sigmoid = 0.0001\n",
        "# alpha_tanh = 0.0001\n",
        "# model_relu = neural_network(ReLU, ReLU_derivative, alpha_relu, input_dim, output_dim, hiddenLayers)\n",
        "# model_sigmoid = neural_network(sigmoid, sigmoid_derivative, alpha_sigmoid, input_dim, output_dim, hiddenLayers)\n",
        "# model_tanh = neural_network(tan_h, tan_h_derivative, alpha_tanh, input_dim, output_dim, hiddenLayers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KWapI2qWI6h7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# error_list_relu = []\n",
        "# iter_list = []\n",
        "# for i in range(400):\n",
        "#         outputs = model_relu.forwardPropagation(X_train)\n",
        "#         c_entropy = crossEntropy(outputs[-1], oneHot)\n",
        "# #         print(i, c_entropy)\n",
        "#         error_list_relu.append(c_entropy)\n",
        "#         iter_list.append(i)\n",
        "#         model_relu.backPropagation(oneHot, X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SHWsBD_CqIoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# error_list_sigmoid = []\n",
        "# iter_list = []\n",
        "# for i in range(400):\n",
        "#         outputs = model_sigmoid.forwardPropagation(X_train)\n",
        "#         c_entropy = crossEntropy(outputs[-1], oneHot)\n",
        "# #         print(i, c_entropy)\n",
        "#         error_list_sigmoid.append(c_entropy)\n",
        "#         iter_list.append(i)\n",
        "#         model_sigmoid.backPropagation(oneHot, X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXuw8wqjpLH8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# error_list_tanh = []\n",
        "# iter_list = []\n",
        "# for i in range(400):\n",
        "#         outputs = model_tanh.forwardPropagation(X_train)\n",
        "#         c_entropy = crossEntropy(outputs[-1], oneHot)\n",
        "# #         print(i, c_entropy)\n",
        "#         error_list_tanh.append(c_entropy)\n",
        "#         iter_list.append(i)\n",
        "#         model_tanh.backPropagation(oneHot, X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QB7NqyrHGTR8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plotting Model Cost w.r.t Iterations \n",
        "## <font color = \"blue\">Models :  </font>\n",
        "\n",
        "\n",
        "1.   **ReLU**\n",
        "2.   **Sigmoid**\n",
        "3.   **Tanh**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QafOB8ldd7MY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots(figsize=(12,6))\n",
        "# ax.scatter(iter_list, error_list_relu, color=\"blue\",marker = '.', s= 30, label = 'ReLU')\n",
        "# ax.scatter(iter_list, error_list_sigmoid, color=\"black\",marker = '.', s= 30, label = 'Sigmoid')\n",
        "# ax.scatter(iter_list, error_list_tanh, color=\"red\",marker = '.', s= 30, label = 'Tanh')\n",
        "# plt.legend()\n",
        "# plt.xlabel(\"Iterations\")\n",
        "# plt.ylabel(\"Cross Entropy Error\")\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2ubD-xJHAiN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Observation : \n",
        "##  <font color = \"blue\">ReLU and Tanh are performing exceptionally well with the current scenario. </font>\n",
        "##  <font color = \"blue\">Surprisingly Tanh is performing better than ReLU in many runs. </font>\n",
        "###  <font color = \"blue\">Though it is expected that ReLU will perform better than Tanh for most of the problems but it is not a guaranteed result. It is infered that the hyper parameters and the architecture favours Tanh</font>"
      ]
    },
    {
      "metadata": {
        "id": "0hAbi2gEH1w3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(\"Minimum Error by ReLU : \", np.min(error_list_relu))\n",
        "# print(\"Minimum Error by Tanh : \", np.min(error_list_tanh))\n",
        "# print(\"Minimum Error by Sigmoid : \", np.min(error_list_sigmoid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ThOarTPSYoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Relu accuracy\n",
        "# val_output = model_relu.forwardPropagation(X_validate)\n",
        "# predictions = np.argmax(val_output[-1], axis=1)\n",
        "# y_validate = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "# print(\"RELU ACCURACY : \", (np.mean(predictions == y_validate))*100)\n",
        "\n",
        "# # Sigmoid accuracy\n",
        "# val_output = model_sigmoid.forwardPropagation(X_validate)\n",
        "# predictions = np.argmax(val_output[-1], axis=1)\n",
        "# y_validate = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "# print(\"SIGMOID ACCURACY : \", (np.mean(predictions == y_validate))*100)\n",
        "\n",
        "# # Tanh accuracy\n",
        "# val_output = model_tanh.forwardPropagation(X_validate)\n",
        "# predictions = np.argmax(val_output[-1], axis=1)\n",
        "# y_validate = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "# print(\"TANH ACCURACY : \", (np.mean(predictions == y_validate))*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kks_EHeWmK5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Mini Batch \n",
        "# model_relu1 = neural_network(ReLU, ReLU_derivative, alpha_relu, input_dim, output_dim, hiddenLayers)\n",
        "# batchSize = 32\n",
        "# batchCount = int(X_train.shape[0] / batchSize)\n",
        "# miniBatchError = []\n",
        "# miniBatchIterList = []\n",
        "# for iters in range(400):\n",
        "#   startIndex = 0\n",
        "#   endIndex = 32\n",
        "#   miniBatchIterList.append(iters)\n",
        "#   for batchNo in range(batchCount):\n",
        "#     X_miniBatch = X_train[startIndex:endIndex]\n",
        "#     outputs = model_relu1.forwardPropagation(X_miniBatch)\n",
        "# #     print(outputs[-1])\n",
        "#     y_miniBatch = oneHot[startIndex:endIndex]\n",
        "#     if(batchNo == batchCount - 1):\n",
        "#       c_entropy = crossEntropy(outputs[-1], y_miniBatch)\n",
        "#       miniBatchError.append(c_entropy)\n",
        "#     model_relu1.backPropagation(y_miniBatch, X_miniBatch)\n",
        "#     startIndex = endIndex\n",
        "#     endIndex += batchSize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kW_xCboikIcp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots(figsize=(12,6))\n",
        "# ax.scatter(miniBatchIterList, miniBatchError, color=\"blue\",marker = '.', s= 30, label = 'ReLU')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpNe3ISVjJzl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# val_output = model_relu1.forwardPropagation(X_validate)\n",
        "# predictions = np.argmax(val_output[-1], axis=1)\n",
        "# y_validate = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
        "# print(\"RELU ACCURACY : \", (np.mean(predictions == y_validate))*100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rxD_kX_cqoMl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(type):\n",
        "#   if(type == \"batch\"):\n",
        "#     train(\"batch\")\n",
        "#   else:\n",
        "#     train(\"miniBatch\")\n",
        "  test_data = pd.read_csv(\"/content/gdrive/My Drive/SMAI_Assignments_colab/apparel-test.csv\")\n",
        "  test_normalized = StandardScaler().fit_transform(test_data)\n",
        "  test(test_normalized)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oKm8E29jgDDY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# main(\"batch\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hZZVj4IBGGD",
        "colab_type": "code",
        "outputId": "eaaf5873-9a80-463b-dd9a-93f8c1eae7e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "main(\"miniBatch\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
            "  return umr_maximum(a, axis, None, out, keepdims)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "odkj6imVdMTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}