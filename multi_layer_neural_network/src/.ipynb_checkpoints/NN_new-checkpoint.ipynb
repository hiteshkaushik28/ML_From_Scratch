{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23141,
     "status": "ok",
     "timestamp": 1551179599732,
     "user": {
      "displayName": "Hitesh Kaushik",
      "photoUrl": "",
      "userId": "12393140368704698022"
     },
     "user_tz": -330
    },
    "id": "op5dMsL9MSqY",
    "outputId": "8946a7fa-c748-4a61-ae44-19270dd7f93b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GafBLbHwI6hI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "eps = np.finfo(float).eps\n",
    "import operator\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5LtEa8WoLm6f"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdXdHYDJI6hM"
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"/content/gdrive/My Drive/SMAI_Assignments_colab/apparel-trainval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ue7RdocmI6hQ"
   },
   "outputs": [],
   "source": [
    "def process_data():\n",
    "#     data = pd.read_csv(path)\n",
    "    y = data[['label']].values\n",
    "    X = data.drop(data.columns[0], axis = 1)\n",
    "    X_normalized = StandardScaler().fit_transform(X)\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(X_normalized, y, test_size = 0.2, random_state=42)\n",
    "    return X_train, X_validate, y_train, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3415,
     "status": "ok",
     "timestamp": 1551184350207,
     "user": {
      "displayName": "Hitesh Kaushik",
      "photoUrl": "",
      "userId": "12393140368704698022"
     },
     "user_tz": -330
    },
    "id": "O06_u7a7I6hT",
    "outputId": "1d3d05a8-7451-4ad1-f5d0-24e33132b13c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X_train, X_validate, y_train, y_validate = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4U6OVaoII6hZ"
   },
   "outputs": [],
   "source": [
    "class neural_network(object):\n",
    "    def __init__(self, activation_func, activation_func_derivative, alpha, ILS, OLS, hiddenLayers):\n",
    "        self.weightMatrix = []\n",
    "        self.allInputs = []\n",
    "        self.allOutputs = []\n",
    "        self.inputLayerSize = ILS\n",
    "        self.outputLayerSize = OLS\n",
    "        self.hiddenLayers = hiddenLayers\n",
    "        self.activation_func = activation_func\n",
    "        self.activation_func_derivative = activation_func_derivative\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if(len(self.hiddenLayers) == 0):\n",
    "            bound = np.sqrt(1./(self.inputLayerSize))\n",
    "            self.weightMatrix.append(np.random.uniform(-bound, bound, (self.inputLayerSize, self.outputLayerSize)))\n",
    "        else:\n",
    "            bound = np.sqrt(1./(self.hiddenLayers[0]))\n",
    "            self.weightMatrix.append(np.random.uniform(-bound, bound, (self.inputLayerSize, self.hiddenLayers[0])))\n",
    "            for x in range(1,len(self.hiddenLayers)):\n",
    "                bound = np.sqrt(1./(self.hiddenLayers[x]))\n",
    "                self.weightMatrix.append(np.random.uniform(-bound, bound, (self.hiddenLayers[x-1], self.hiddenLayers[x])))\n",
    "            self.weightMatrix.append(np.random.uniform(-bound, bound, (self.hiddenLayers[len(self.hiddenLayers)-1], self.outputLayerSize)))\n",
    "    \n",
    "    def forwardPropagation(self, Input):\n",
    "        self.allInputs = []\n",
    "        self.allOutputs = []\n",
    "        for i in range(0, len(self.weightMatrix)):\n",
    "            \n",
    "            if(i == 0):\n",
    "                self.allInputs.append(np.matmul(Input,self.weightMatrix[0]))\n",
    "            else:\n",
    "                self.allInputs.append(np.matmul(self.allOutputs[i-1],self.weightMatrix[i]))\n",
    "            \n",
    "            if (i == len(self.weightMatrix)-1):\n",
    "                self.allOutputs.append(stable_softmax(self.allInputs[i]))\n",
    "            else:\n",
    "                self.allOutputs.append(self.activation_func(self.allInputs[i]))\n",
    "        return self.allOutputs\n",
    "    \n",
    "    def backPropagation(self, encoded, input):\n",
    "        delta_list = [0]*len(self.weightMatrix)\n",
    "        for i in reversed(range(len(self.weightMatrix))):\n",
    "            if(i == len(self.weightMatrix) - 1):\n",
    "                delta = np.multiply((self.allOutputs[i] - encoded), softmax_derivative(self.allInputs[i]))\n",
    "            else:\n",
    "                delta = np.multiply(np.dot(delta_list[i+1], self.weightMatrix[i+1].T), self.activation_func_derivative(self.allInputs[i]))\n",
    "            delta_list[i] = delta\n",
    "        for i in range(0, len(self.weightMatrix)):\n",
    "            if(i == 0):\n",
    "                self.weightMatrix[i] -= self.alpha * np.dot(input.T, delta_list[0])\n",
    "            else:\n",
    "                self.weightMatrix[i] -= self.alpha * np.dot(self.allOutputs[i-1].T, delta_list[i])\n",
    "                \n",
    "    def predictionOnTest(model_test, data):\n",
    "      outputs = model_test.forwardPropagation(data)\n",
    "      predictions = np.argmax(outputs[-1], axis=1)\n",
    "      return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4xc6ImMI6hf"
   },
   "outputs": [],
   "source": [
    "def oneHotEncode(data):\n",
    "    n_classes = len(np.unique(data))\n",
    "    targets = np.array(data).reshape(-1)\n",
    "    return np.eye(n_classes)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7mKBPi1I6hi"
   },
   "outputs": [],
   "source": [
    "# oneHot = oneHotEncode(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjBus-NPI6hn"
   },
   "outputs": [],
   "source": [
    "def crossEntropy(prediction, target, epsilon=1e-12):\n",
    "    prediction = np.clip(prediction, epsilon, 1. - epsilon)\n",
    "    N = prediction.shape[0]\n",
    "    c_entropy = -np.sum(target * np.log(prediction+1e-9))/N\n",
    "    return c_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgFhBniXI6hr"
   },
   "outputs": [],
   "source": [
    "def ReLU_derivative(matrix):\n",
    "    matrix[matrix > 0] = 1\n",
    "    matrix[matrix <= 0] = 0\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dNB_pVpI6hv"
   },
   "outputs": [],
   "source": [
    " def ReLU(matrix):\n",
    "        return np.maximum(matrix, 0.0, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRe5pw-iXIEh"
   },
   "outputs": [],
   "source": [
    "def sigmoid(matrix):\n",
    "  sigm = 1. / (1. + np.exp(-matrix))\n",
    "  return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Yen6jdkYDaR"
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(matrix):\n",
    "  sigm = sigmoid(matrix)\n",
    "  return sigm * (1. - sigm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6B-_n8tbX5Eh"
   },
   "outputs": [],
   "source": [
    "def tan_h(matrix):\n",
    "  return np.tanh(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YDFI0gdYbGQ"
   },
   "outputs": [],
   "source": [
    "def tan_h_derivative(matrix):\n",
    "  return 1.0 - np.tanh(matrix)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJgpUza5I6hy"
   },
   "outputs": [],
   "source": [
    "def stable_softmax(matrix):\n",
    "        shift_x = matrix - np.array([np.max(matrix, axis=1)]).T\n",
    "        exps = np.exp(shift_x)\n",
    "        return exps / np.array([np.sum(exps, axis=1)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HTb9fMSI6h1"
   },
   "outputs": [],
   "source": [
    "def softmax_derivative(matrix):\n",
    "    temp = stable_softmax(matrix)\n",
    "    return temp*(1 - temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHrSXoc0U8eF"
   },
   "outputs": [],
   "source": [
    "def trainOnRelu(model_relu, epochs):\n",
    "    error_list_relu = []\n",
    "    iter_list = []\n",
    "    for i in range(epochs):\n",
    "      print(\"relu : \", i)\n",
    "      outputs = model_relu.forwardPropagation(X_train)\n",
    "      c_entropy = crossEntropy(outputs[-1], oneHot)\n",
    "      error_list_relu.append(c_entropy)\n",
    "      iter_list.append(i)\n",
    "      model_relu.backPropagation(oneHot, X_train)\n",
    "    return error_list_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9DJBUjRVqEl"
   },
   "outputs": [],
   "source": [
    "def trainOnSigmoid(model_sigmoid, epochs):\n",
    "    error_list_sigmoid = []\n",
    "    iter_list = []\n",
    "    for i in range(epochs):\n",
    "      print(\"sigmoid : \", i)\n",
    "      outputs = model_sigmoid.forwardPropagation(X_train)\n",
    "      c_entropy = crossEntropy(outputs[-1], oneHot)\n",
    "      error_list_sigmoid.append(c_entropy)\n",
    "      iter_list.append(i)\n",
    "      model_sigmoid.backPropagation(oneHot, X_train)\n",
    "    return error_list_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sFg4tuuV_q6"
   },
   "outputs": [],
   "source": [
    "def trainOnTanh(model_tanh, epochs):\n",
    "    error_list_tanh = []\n",
    "    iter_list = []\n",
    "    for i in range(epochs):\n",
    "      print(\"tanh : \", i)\n",
    "      outputs = model_tanh.forwardPropagation(X_train)\n",
    "      c_entropy = crossEntropy(outputs[-1], oneHot)\n",
    "      error_list_tanh.append(c_entropy)\n",
    "      iter_list.append(i)\n",
    "      model_tanh.backPropagation(oneHot, X_train)\n",
    "    return error_list_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0aHAbE8XNCe"
   },
   "outputs": [],
   "source": [
    "def miniBatchTrain(miniBatchModel, batchSize, epochs):\n",
    "    batchCount = int(X_train.shape[0] / batchSize)\n",
    "    miniBatchError = []\n",
    "    for iters in range(epochs):\n",
    "      print(\"miniBatch : \", iters)\n",
    "      startIndex = 0\n",
    "      endIndex = batchSize\n",
    "      for batchNo in range(batchCount):\n",
    "        X_miniBatch = X_train[startIndex:endIndex]\n",
    "        outputs = miniBatchModel.forwardPropagation(X_miniBatch)\n",
    "        y_miniBatch = oneHot[startIndex:endIndex]\n",
    "        if(batchNo == batchCount - 1):\n",
    "          c_entropy = crossEntropy(outputs[-1], y_miniBatch)\n",
    "          miniBatchError.append(c_entropy)\n",
    "        miniBatchModel.backPropagation(y_miniBatch, X_miniBatch)\n",
    "        startIndex = endIndex\n",
    "        endIndex += batchSize\n",
    "    return miniBatchError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o80lhw8EbRvn"
   },
   "outputs": [],
   "source": [
    "def plotBatch(error_list_relu, error_list_sigmoid, error_list_tanh, iter_list):\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.scatter(iter_list, error_list_relu, color=\"blue\",marker = '.', s= 30, label = 'ReLU')\n",
    "    ax.scatter(iter_list, error_list_sigmoid, color=\"black\",marker = '.', s= 30, label = 'Sigmoid')\n",
    "    ax.scatter(iter_list, error_list_tanh, color=\"red\",marker = '.', s= 30, label = 'Tanh')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cross Entropy Error\")\n",
    "    plt.title(\"Iterations vs Model Cost : Batch Mode\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8koX_XIbm9G"
   },
   "outputs": [],
   "source": [
    "def plotMiniBatch(error_list_relu, error_list_sigmoid, error_list_tanh, iter_list):\n",
    "    fig1, ax1 = plt.subplots(figsize=(12,6))\n",
    "    ax1.scatter(iter_list, error_list_relu, color=\"blue\",marker = '.', s= 30, label = 'ReLU')\n",
    "    ax1.scatter(iter_list, error_list_sigmoid, color=\"black\",marker = '.', s= 30, label = 'Sigmoid')\n",
    "    ax1.scatter(iter_list, error_list_tanh, color=\"red\",marker = '.', s= 30, label = 'Tanh')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cross Entropy Error\")\n",
    "    plt.title(\"Iterations vs Model Cost : Mini Batch Mode\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4efG_GXPvna"
   },
   "outputs": [],
   "source": [
    "def train(trainType):\n",
    "    hiddenLayers = [60,60]\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    alpha_relu = 0.0001\n",
    "    alpha_sigmoid = 0.0001\n",
    "    alpha_tanh = 0.0001\n",
    "    \n",
    "    if(trainType == \"batch\"):\n",
    "      model_relu = neural_network(ReLU, ReLU_derivative, alpha_relu, input_dim, output_dim, hiddenLayers)\n",
    "      model_sigmoid = neural_network(sigmoid, sigmoid_derivative, alpha_sigmoid, input_dim, output_dim, hiddenLayers)\n",
    "      model_tanh = neural_network(tan_h, tan_h_derivative, alpha_tanh, input_dim, output_dim, hiddenLayers)\n",
    "      epochs = 500\n",
    "      relu_error = trainOnRelu(model_relu, epochs)\n",
    "      sigmoid_error = trainOnSigmoid(model_sigmoid, epochs)\n",
    "      tanh_error = trainOnTanh(model_tanh, epochs)\n",
    "      iter_list = range(0, epochs)\n",
    "      plotBatch(relu_error, sigmoid_error, tanh_error, iter_list)\n",
    "      \n",
    "      print(\"Minimum Error by ReLU : \", np.min(relu_error))\n",
    "      print(\"Minimum Error by Tanh : \", np.min(tanh_error))\n",
    "      print(\"Minimum Error by Sigmoid : \", np.min(sigmoid_error))\n",
    "      \n",
    "      \n",
    "      print(\"\\n\\nACCURACY USING BATCH MODE: \")\n",
    "      # Relu accuracy\n",
    "      val_output = model_relu.forwardPropagation(X_validate)\n",
    "      predictions = np.argmax(val_output[-1], axis=1)\n",
    "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "      print(\"RELU : \", (np.mean(predictions == y_val))*100)\n",
    "\n",
    "      # Sigmoid accuracy\n",
    "      val_output = model_sigmoid.forwardPropagation(X_validate)\n",
    "      predictions = np.argmax(val_output[-1], axis=1)\n",
    "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "      print(\"SIGMOID : \", (np.mean(predictions == y_val))*100)\n",
    "\n",
    "      # Tanh accuracy\n",
    "      val_output = model_tanh.forwardPropagation(X_validate)\n",
    "      predictions = np.argmax(val_output[-1], axis=1)\n",
    "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "      print(\"TANH : \", (np.mean(predictions == y_val))*100)\n",
    "      \n",
    "      # saving weights\n",
    "      weight_batch_relu = model_relu.weightMatrix\n",
    "      np.save('weights_batch_relu', weight_batch_relu)\n",
    "      \n",
    "      weight_batch_sigmoid = model_sigmoid.weightMatrix\n",
    "      np.save('weights_batch_sigmoid', weight_batch_sigmoid)\n",
    "      \n",
    "      weight_batch_relu = model_relu.weightMatrix\n",
    "      np.save('weights_batch_tanh', weight_batch_tanh)\n",
    "      \n",
    "      \n",
    "      \n",
    "    elif(trainType == \"miniBatch\"):\n",
    "      iters = 200\n",
    "      batchSize = 32\n",
    "      iter_list = range(0, iters)\n",
    "      model_miniBatch_relu = neural_network(ReLU, ReLU_derivative, alpha_relu, input_dim, output_dim, hiddenLayers)\n",
    "      model_miniBatch_sigmoid = neural_network(sigmoid, sigmoid_derivative, alpha_sigmoid, input_dim, output_dim, hiddenLayers)\n",
    "      model_miniBatch_tanh = neural_network(tan_h, tan_h_derivative, alpha_tanh, input_dim, output_dim, hiddenLayers)\n",
    "      miniBatchError_relu = miniBatchTrain(model_miniBatch_relu, batchSize, iters)\n",
    "      miniBatchError_sigmoid = miniBatchTrain(model_miniBatch_sigmoid, batchSize, iters)\n",
    "      miniBatchError_tanh = miniBatchTrain(model_miniBatch_tanh, batchSize, iters)\n",
    "      plotMiniBatch(miniBatchError_relu, miniBatchError_sigmoid, miniBatchError_tanh, iter_list)\n",
    "      \n",
    "      print(\"\\n\\nACCURACY USING MINI BATCH: \")\n",
    "      val_output = model_miniBatch_relu.forwardPropagation(X_validate)\n",
    "      predictions = np.argmax(val_output[-1], axis=1)\n",
    "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "      print(\"RELU : \", (np.mean(predictions == y_val))*100)\n",
    "      \n",
    "      val_output = model_miniBatch_sigmoid.forwardPropagation(X_validate)\n",
    "      predictions = np.argmax(val_output[-1], axis=1)\n",
    "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "      print(\"SIGMOID : \", (np.mean(predictions == y_val))*100)\n",
    "      \n",
    "      val_output = model_miniBatch_tanh.forwardPropagation(X_validate)\n",
    "      predictions = np.argmax(val_output[-1], axis=1)\n",
    "      y_val = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "      print(\"TANH : \", (np.mean(predictions == y_val))*100)\n",
    "      \n",
    "      # saving weights\n",
    "      weight_mini_batch_relu = model_miniBatch_relu.weightMatrix\n",
    "      np.save('weights_mini_batch_relu', weight_mini_batch_relu)\n",
    "      \n",
    "      weight_mini_batch_sigmoid = model_miniBatch_sigmoid.weightMatrix\n",
    "      np.save('weights_mini_batch_sigmoid', weight_mini_batch_sigmoid)\n",
    "      \n",
    "      weight_mini_batch_tanh = model_miniBatch_tanh.weightMatrix\n",
    "      np.save('weights_mini_batch_tanh', weight_mini_batch_tanh)\n",
    "\n",
    "    else:\n",
    "      print(\"Wrong Training Method Provided\")\n",
    "      sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoRLpaO0Jnve"
   },
   "outputs": [],
   "source": [
    "def test(test_data):\n",
    "  alpha_tanh = 0.0001\n",
    "  input_dim = test_data.shape[1]\n",
    "  output_dim = 10\n",
    "  hiddenLayers = [60, 60]\n",
    "  model_test = neural_network(tan_h, tan_h_derivative, alpha_tanh, input_dim, output_dim, hiddenLayers)\n",
    "  weights = np.load('weights_mini_batch_tanh.npy')\n",
    "  weights = loaded_best_list.tolist()\n",
    "  model_test.weightMatrix = weights\n",
    "  predictions = model_test.predictionOnTest(data)\n",
    "  fileData = [[i] for i in predictions]\n",
    "  with open('2018201057_prediction.csv', 'w') as outputFile:\n",
    "    writer = csv.writer(outputFile)\n",
    "    writer.writerows(fileData)\n",
    "  outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1cwoUuMI6h4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hiddenLayers = [60,60]\n",
    "# input_dim = X_train.shape[1]\n",
    "# output_dim = len(np.unique(y_train))\n",
    "# alpha_relu = 0.0001\n",
    "# alpha_sigmoid = 0.0001\n",
    "# alpha_tanh = 0.0001\n",
    "# model_relu = neural_network(ReLU, ReLU_derivative, alpha_relu, input_dim, output_dim, hiddenLayers)\n",
    "# model_sigmoid = neural_network(sigmoid, sigmoid_derivative, alpha_sigmoid, input_dim, output_dim, hiddenLayers)\n",
    "# model_tanh = neural_network(tan_h, tan_h_derivative, alpha_tanh, input_dim, output_dim, hiddenLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWapI2qWI6h7"
   },
   "outputs": [],
   "source": [
    "# error_list_relu = []\n",
    "# iter_list = []\n",
    "# for i in range(400):\n",
    "#         outputs = model_relu.forwardPropagation(X_train)\n",
    "#         c_entropy = crossEntropy(outputs[-1], oneHot)\n",
    "# #         print(i, c_entropy)\n",
    "#         error_list_relu.append(c_entropy)\n",
    "#         iter_list.append(i)\n",
    "#         model_relu.backPropagation(oneHot, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHWsBD_CqIoO"
   },
   "outputs": [],
   "source": [
    "# error_list_sigmoid = []\n",
    "# iter_list = []\n",
    "# for i in range(400):\n",
    "#         outputs = model_sigmoid.forwardPropagation(X_train)\n",
    "#         c_entropy = crossEntropy(outputs[-1], oneHot)\n",
    "# #         print(i, c_entropy)\n",
    "#         error_list_sigmoid.append(c_entropy)\n",
    "#         iter_list.append(i)\n",
    "#         model_sigmoid.backPropagation(oneHot, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXuw8wqjpLH8"
   },
   "outputs": [],
   "source": [
    "# error_list_tanh = []\n",
    "# iter_list = []\n",
    "# for i in range(400):\n",
    "#         outputs = model_tanh.forwardPropagation(X_train)\n",
    "#         c_entropy = crossEntropy(outputs[-1], oneHot)\n",
    "# #         print(i, c_entropy)\n",
    "#         error_list_tanh.append(c_entropy)\n",
    "#         iter_list.append(i)\n",
    "#         model_tanh.backPropagation(oneHot, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB7NqyrHGTR8"
   },
   "source": [
    "# Plotting Model Cost w.r.t Iterations \n",
    "## <font color = \"blue\">Models :  </font>\n",
    "\n",
    "\n",
    "1.   **ReLU**\n",
    "2.   **Sigmoid**\n",
    "3.   **Tanh**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QafOB8ldd7MY"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,6))\n",
    "# ax.scatter(iter_list, error_list_relu, color=\"blue\",marker = '.', s= 30, label = 'ReLU')\n",
    "# ax.scatter(iter_list, error_list_sigmoid, color=\"black\",marker = '.', s= 30, label = 'Sigmoid')\n",
    "# ax.scatter(iter_list, error_list_tanh, color=\"red\",marker = '.', s= 30, label = 'Tanh')\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Iterations\")\n",
    "# plt.ylabel(\"Cross Entropy Error\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2ubD-xJHAiN"
   },
   "source": [
    "# Observation : \n",
    "##  <font color = \"blue\">ReLU and Tanh are performing exceptionally well with the current scenario. </font>\n",
    "##  <font color = \"blue\">Surprisingly Tanh is performing better than ReLU in many runs. </font>\n",
    "###  <font color = \"blue\">Though it is expected that ReLU will perform better than Tanh for most of the problems but it is not a guaranteed result. It is infered that the hyper parameters and the architecture favours Tanh</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hAbi2gEH1w3"
   },
   "outputs": [],
   "source": [
    "# print(\"Minimum Error by ReLU : \", np.min(error_list_relu))\n",
    "# print(\"Minimum Error by Tanh : \", np.min(error_list_tanh))\n",
    "# print(\"Minimum Error by Sigmoid : \", np.min(error_list_sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ThOarTPSYoL"
   },
   "outputs": [],
   "source": [
    "# # Relu accuracy\n",
    "# val_output = model_relu.forwardPropagation(X_validate)\n",
    "# predictions = np.argmax(val_output[-1], axis=1)\n",
    "# y_validate = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "# print(\"RELU ACCURACY : \", (np.mean(predictions == y_validate))*100)\n",
    "\n",
    "# # Sigmoid accuracy\n",
    "# val_output = model_sigmoid.forwardPropagation(X_validate)\n",
    "# predictions = np.argmax(val_output[-1], axis=1)\n",
    "# y_validate = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "# print(\"SIGMOID ACCURACY : \", (np.mean(predictions == y_validate))*100)\n",
    "\n",
    "# # Tanh accuracy\n",
    "# val_output = model_tanh.forwardPropagation(X_validate)\n",
    "# predictions = np.argmax(val_output[-1], axis=1)\n",
    "# y_validate = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "# print(\"TANH ACCURACY : \", (np.mean(predictions == y_validate))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kks_EHeWmK5"
   },
   "outputs": [],
   "source": [
    "# # Mini Batch \n",
    "# model_relu1 = neural_network(ReLU, ReLU_derivative, alpha_relu, input_dim, output_dim, hiddenLayers)\n",
    "# batchSize = 32\n",
    "# batchCount = int(X_train.shape[0] / batchSize)\n",
    "# miniBatchError = []\n",
    "# miniBatchIterList = []\n",
    "# for iters in range(400):\n",
    "#   startIndex = 0\n",
    "#   endIndex = 32\n",
    "#   miniBatchIterList.append(iters)\n",
    "#   for batchNo in range(batchCount):\n",
    "#     X_miniBatch = X_train[startIndex:endIndex]\n",
    "#     outputs = model_relu1.forwardPropagation(X_miniBatch)\n",
    "# #     print(outputs[-1])\n",
    "#     y_miniBatch = oneHot[startIndex:endIndex]\n",
    "#     if(batchNo == batchCount - 1):\n",
    "#       c_entropy = crossEntropy(outputs[-1], y_miniBatch)\n",
    "#       miniBatchError.append(c_entropy)\n",
    "#     model_relu1.backPropagation(y_miniBatch, X_miniBatch)\n",
    "#     startIndex = endIndex\n",
    "#     endIndex += batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kW_xCboikIcp"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,6))\n",
    "# ax.scatter(miniBatchIterList, miniBatchError, color=\"blue\",marker = '.', s= 30, label = 'ReLU')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpNe3ISVjJzl"
   },
   "outputs": [],
   "source": [
    "# val_output = model_relu1.forwardPropagation(X_validate)\n",
    "# predictions = np.argmax(val_output[-1], axis=1)\n",
    "# y_validate = np.argmax(oneHotEncode(y_validate), axis = 1)\n",
    "# print(\"RELU ACCURACY : \", (np.mean(predictions == y_validate))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxD_kX_cqoMl"
   },
   "outputs": [],
   "source": [
    "def main(type):\n",
    "#   if(type == \"batch\"):\n",
    "#     train(\"batch\")\n",
    "#   else:\n",
    "#     train(\"miniBatch\")\n",
    "    test_data = pd.read_csv(\"../input/apparel-test.csv\")\n",
    "    test_normalized = StandardScaler().fit_transform(test_data)\n",
    "    test(test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKm8E29jgDDY"
   },
   "outputs": [],
   "source": [
    "# main(\"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "-hZZVj4IBGGD",
    "outputId": "68c8cde6-9869-4083-b26a-fa4c5c3201f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miniBatch :  0\n",
      "miniBatch :  1\n",
      "miniBatch :  2\n",
      "miniBatch :  3\n",
      "miniBatch :  4\n",
      "miniBatch :  5\n",
      "miniBatch :  6\n",
      "miniBatch :  7\n",
      "miniBatch :  8\n",
      "miniBatch :  9\n",
      "miniBatch :  10\n",
      "miniBatch :  11\n",
      "miniBatch :  12\n",
      "miniBatch :  13\n",
      "miniBatch :  14\n",
      "miniBatch :  15\n",
      "miniBatch :  16\n",
      "miniBatch :  17\n",
      "miniBatch :  18\n",
      "miniBatch :  19\n",
      "miniBatch :  20\n",
      "miniBatch :  21\n",
      "miniBatch :  22\n",
      "miniBatch :  23\n",
      "miniBatch :  24\n",
      "miniBatch :  25\n",
      "miniBatch :  26\n",
      "miniBatch :  27\n",
      "miniBatch :  28\n",
      "miniBatch :  29\n",
      "miniBatch :  30\n",
      "miniBatch :  31\n",
      "miniBatch :  32\n",
      "miniBatch :  33\n",
      "miniBatch :  34\n"
     ]
    }
   ],
   "source": [
    "main(\"miniBatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odkj6imVdMTx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
