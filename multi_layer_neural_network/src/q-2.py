# -*- coding: utf-8 -*-
"""q-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wx-R0QSqLUujaYN41wRUDQVpRrhi0z76
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

"""# Data Handling:
### Neural networks work internally with numeric data, binary data and categorical data must be encoded in numeric form. Additionally, experience has shown that in most cases numeric data, such as a person's age, should be normalized.
### When encoding binary and categorical data, there are four cases you must deal with: independent (x) binary data, dependent (y) binary data, independent (x) categorical data and dependent (y) categorical data. An example of independent binary data is a predictor variable, sex, which can take one of two values: "male" or "female." For such data I recommend encoding the two possible values as -1.0 and +1.0.
### $male   = -1.0$
### $female = +1.0$
### There's convincing research that indicates a $-1.0, +1.0$ scheme is superior to a simple $0.0, 1.0$ scheme for independent binary variables.
### An example of independent categorical data is a predictor variable community, which can take values $"suburban," "rural" or "city".$
### One-Hot Encoding Scheme : 
#### Considering we have the numeric representation of any categorical attribute with m labels (after transformation), the one-hot encoding scheme, encodes or transforms the attribute into m binary features which can only contain a value of $1\  or\ 0$. Each observation in the categorical feature is thus converted into a vector of size m with only one of the values as 1 (indicating it as active). 
### Dummy Encoding Scheme : 
#### The dummy coding scheme is similar to the one-hot encoding scheme, except in the case of dummy coding scheme, when applied on a categorical feature with m distinct labels, we get $m - 1$ binary features. Thus each value of the categorical variable gets converted into a vector of size$m - 1$. The extra feature is completely disregarded and thus if the category values range from ${0, 1, …, m-1}$ the $0th$ or the $m - 1th$ feature column is dropped and corresponding category values are usually represented by a vector of all zeros $(0)$.

# Network Architecture :
## Input Layer :
### With respect to the number of neurons comprising this layer, this parameter is completely and uniquely determined once we know the shape of our training data. Specifically, the number of neurons comprising that layer is equal to the number of features (columns) in your data. Some NN configurations add one additional node for a bias term.
## Hidden Layers :
### How many hidden layers? Well if our data is linearly separable (which we often know by the time we begin coding a NN) then we don't need any hidden layers at all. Of course, we don't need an NN to resolve our data either, but it will still do the job.
###  One hidden layer is sufficient for the large majority of problems.So what about size of the hidden layer(s)--how many neurons? There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'.
## Output Layer :
### Like the Input layer, every NN has exactly one output layer. Determining its size (number of neurons) is simple; it is completely determined by the chosen model configuration.

### Is your NN going running in Machine Mode or Regression Mode (the ML convention of using a term that is also used in statistics but assigning a different meaning to it is very confusing). Machine mode: returns a class label (e.g., "Premium Account"/"Basic Account"). Regression Mode returns a value (e.g., price).
### If the NN is a regressor, then the output layer has a single node.
### If the NN is a classifier, then it also has a single node unless softmax is used in which case the output layer has one node per class label in your model.
### In summary, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers.

## Activation Function :
### Our problem is related to prediction of a value, which ofcourse doesn't have any bounds. Because of this reason, our activation functions need to be linear. 
### Suppose we have 3 hidden layers, then at each layer, there will be a linear activation function.

# Handling  Missing Data : 
### 1. Using Distribution of the attribute values : In this approach, the distribution of possible missing value is estimated and corresponding model predictions are combined probabilistically. At the time of testing, a test instance with missing value is split into branches according to the portions of training examples falling into those branches and goes down to leaves.
### 2. Another approach is to treat missing value as a normal value, ‘Null’ in tree construction and testing process. As values might be missing for certain reason, it might be good idea to assign a special value. There are two drawbacks of this method. Firstly, it does not try to identify the original known values as missing values are considered as equally as a original value. Another disadvantage is, in case of more than one actual missing values and replacing all of them by one value ‘null’ may not be correct. Also, subtrees can be built under the null branch, oddly suggesting that the missing value is more discriminating then the known value. The advantage of this method is that all the records can be used for tree building and it is very simple to implement. This approach handles the missing at the training time as well.
### Simply discarding the record with missing values. This approach may lead to loss of important data.
"""

